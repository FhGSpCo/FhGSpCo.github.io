<!DOCTYPE html>
<html>
	<head>
		<title>StyleMelGAN: An Efficient High-Fidelity Adversarial Vocoder with Temporal Adaptive Normalization</title>
	</head>
	<body>
		<div class="container">
    		<div class="blurb">
                <h1>StyleMelGAN: An Efficient High-Fidelity Adversarial Vocoder with Temporal Adaptive Normalization</h1>		
	        <section>
         <h3>Abstract:</h3><p>In recent years, neural vocoders have surpassed classical speech synthesis approaches in terms of naturalness and perceptual quality of the synthesized speech signals.
         The best results can be achieved with computationally-heavy neural vocoders like WaveNet and WaveGlow, while light-weight architectures based on Generative Adverserial Networks, e.g. MelGAN and Parallel WaveGAN, are still inferior in terms of the perceptual quality.
         In this context we propose StyleMelGAN, a light-weight neural vocoder, allowing synthesis of high-fidelity speech with low computational complexity.
         StyleMelGAN is a fully convolutional, feed-forward model that uses Temporal Adaptive DEnormalization (TADE) to style a low-dimensional noise vector via the acoustic features of the target speech waveform.
         The architecture allows for highly parallelizable generation, several times faster than real time on both CPUs and GPUs.
         For efficient and fast training we use a multi-scale spectral reconstruction loss together with an adversarial loss calculated by multiple discriminators evaluating the speech signal in multiple frequency bands and with random windowing.
         MUSHRA and P.800 listening tests show that StyleMelGAN outperforms known existing neural vocoders in both copy-synthesis and TTS scenarios.
               </p> 
               </section>
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
		<footer>
    		<ul>
        		<li><a href="mailto:hankquinlanhub@gmail.com">email</a></li>
        		<li><a href="https://github.com/hankquinlan">github.com/hankquinlan</a></li>
			</ul>
		</footer>
	</body>
</html>
